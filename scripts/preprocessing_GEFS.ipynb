{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This .ipynb contains preprocessing codes for GEFS reforecast\n",
        "and GEFS reanalysis datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ej7j4WL8y_EX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\bobby\\anaconda3\\envs\\research\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        }
      ],
      "source": [
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import cartopy.crs as ccrs  # for plotting map\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh803EA3zynP"
      },
      "source": [
        "# Reanalysis and Reforecast 16 x 19 --> 8 x 11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ffYrjzMVzJ8P"
      },
      "outputs": [],
      "source": [
        "# reading variable files\n",
        "PATH = 'C:\\\\Users\\\\bobby\\\\Desktop\\\\.vscode\\\\1 UROP Research\\\\UROP v2\\\\raw_preprocessing\\\\GEFS\\\\'\n",
        "ds_reanalysis_tp = xr.open_dataset(PATH + 'GEFSv12-Reanalysis_tp_2000_2019.nc') # only 1 single reanalysis tp\n",
        "ds_reforecast_apcp = xr.open_dataset(PATH + 'GEFSv12-Reforecast_apcp_2000_2019.nc') # may add more variables in future\n",
        "\n",
        "# slicing dimensions 8 x 11 grid points\n",
        "ds_reanalysis_tp = ds_reanalysis_tp.sel(lon=slice('102.5', '105.00'), lat=('2.5','2.25','2','1.75','1.5','1.25','1','0.75'))\n",
        "ds_reforecast_apcp = ds_reforecast_apcp.sel(longitude=slice('102.5', '105.00'), latitude=('2.5','2.25','2','1.75','1.5','1.25','1','0.75'))\n",
        "\n",
        "# starting from 2000-01-01 06:00:00 to 2019-12-31 18:00:00, total 29219 time steps\n",
        "ds_reanalysis_tp = ds_reanalysis_tp.isel(time=slice(1, None)) \n",
        "del ds_reanalysis_tp.attrs['history'] # remove long history text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ds_reanalysis_tp.isel(time=0).tp # checking magnitudes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# plotting check between reforecast and reanalysis per timestep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# timestep = 12\n",
        "# p1m = ds_reanalysis_tp.isel(time=timestep).tp\n",
        "# p2m = ds_reforecast_apcp.isel(time=timestep).tp\n",
        "\n",
        "# fig = plt.figure(figsize=(7, 3.5))\n",
        "# p = p1m.plot(x=\"lon\", y=\"lat\", \n",
        "#              subplot_kws={\"projection\": ccrs.PlateCarree()}, transform=ccrs.PlateCarree(), cmap=\"Blues\")\n",
        "# p.axes.coastlines()\n",
        "# p.axes.gridlines(draw_labels=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fig = plt.figure(figsize=(7, 3.5))\n",
        "# p = p2m.plot(x=\"longitude\", y=\"latitude\", \n",
        "#              subplot_kws={\"projection\": ccrs.PlateCarree()}, transform=ccrs.PlateCarree(), cmap=\"Blues\")\n",
        "# p.axes.coastlines()\n",
        "# p.axes.gridlines(draw_labels=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# split into train val test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_train_apcp = ds_reforecast_apcp.sel(time=slice('2000-01-01T06', '2014-01-01T06'))\n",
        "ds_val_apcp = ds_reforecast_apcp.sel(time=slice('2014-01-01T12', '2016-12-31T12'))\n",
        "ds_test_apcp = ds_reforecast_apcp.sel(time=slice('2016-12-31T18', '2019-12-31T18'))\n",
        "\n",
        "\n",
        "y_train = ds_reanalysis_tp.sel(time=slice('2000-01-01T06', '2014-01-01T06'))\n",
        "y_val = ds_reanalysis_tp.sel(time=slice('2014-01-01T12', '2016-12-31T12'))\n",
        "y_test = ds_reanalysis_tp.sel(time=slice('2016-12-31T18', '2019-12-31T18'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: {numpy.datetime64('2012-05-18T00','h'), numpy.datetime64('2012-05-17T18','h'), numpy.datetime64('2012-05-17T12','h'), numpy.datetime64('2012-05-17T06','h')}\n",
            "val: set()\n",
            "test: set()\n"
          ]
        }
      ],
      "source": [
        "# time check for apcp\n",
        "\n",
        "# reforecast_time_check = ds_reforecast_apcp.time.values.astype('datetime64[h]') # 29215\n",
        "# print(reforecast_time_check)\n",
        "# print(len(reforecast_time_check))\n",
        "\n",
        "# official_time_check = np.arange(np.datetime64(\"2000-01-01T06\"), np.datetime64(\"2020-01-01\"), np.timedelta64(6, \"h\")) # 29219\n",
        "# print(official_time_check)\n",
        "# print(len(official_time_check))\n",
        "\n",
        "# print('\\nmissing time steps:')\n",
        "# set(official_time_check) - set(reforecast_time_check) # missing data on 2012-05-17\n",
        "\n",
        "def timecheck(train, val, test):\n",
        "    '''\n",
        "    Return missing time step in train, val and test split\n",
        "    '''\n",
        "    train_timecheck = np.arange(np.datetime64(\"2000-01-01T06\"), np.datetime64(\"2014-01-01T12\"), np.timedelta64(6, \"h\"))\n",
        "    val_timecheck = np.arange(np.datetime64(\"2014-01-01T12\"), np.datetime64(\"2016-12-31T18\"), np.timedelta64(6, \"h\"))\n",
        "    test_timecheck = np.arange(np.datetime64(\"2016-12-31T18\"), np.datetime64(\"2020-01-01T00\"), np.timedelta64(6, \"h\"))\n",
        "    print('train:', set(train_timecheck) - set(train.time.values.astype('datetime64[h]')))\n",
        "    print('val:', set(val_timecheck) - set(val.time.values.astype('datetime64[h]')))\n",
        "    print('test:', set(test_timecheck) - set(test.time.values.astype('datetime64[h]')))\n",
        "    return\n",
        "\n",
        "timecheck(ds_train_apcp, ds_val_apcp, ds_test_apcp)\n",
        "# timecheck(y_train, y_val, y_test) # reanalysis are all ok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "ds_train_apcp = ds_train_apcp.resample(time='6H').asfreq()\n",
        "ds_train_apcp.tp.values = ds_train_apcp.tp.interpolate_na(dim='time')\n",
        "print(not ds_reforecast_apcp.tp.isnull().any()) # True if contains all numeric values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Log transform and normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# log transform and min_max normalization for reforecast_apcp TRAINING DATASET\n",
        "scaler_train_apcp = MinMaxScaler()\n",
        "X_one_col = ds_train_apcp.tp.values.reshape([ds_train_apcp.tp.values.shape[0]*ds_train_apcp.tp.values.shape[1]*ds_train_apcp.tp.values.shape[2], 1])\n",
        "X_one_col = np.log10(X_one_col+1) # 10**X_one_col - 1 to scale back\n",
        "X_one_col_res = scaler_train_apcp.fit_transform(X_one_col) # scaler_train_apcp.inverse_transform(X_one_col_res) to scale back, or use 10**scaler_train_apcp.inverse_transform(X_one_col_res) -1 only\n",
        "ds_train_apcp.tp.values = X_one_col_res.reshape(ds_train_apcp.tp.values.shape)\n",
        "\n",
        "# log transform and min_max normalization for reanalysis_tp TRAINING DATASET\n",
        "scaler_train_tp = MinMaxScaler() \n",
        "y_one_col = y_train.tp.values.reshape([y_train.tp.values.shape[0]*y_train.tp.values.shape[1]*y_train.tp.values.shape[2], 1])\n",
        "y_one_col = np.log10(y_one_col+1) # 10**X_one_col - 1 to scale back\n",
        "y_one_col_res = scaler_train_tp.fit_transform(y_one_col) # scaler_train_apcp.inverse_transform(X_one_col_res) to scale back, or use 10**scaler_train_apcp.inverse_transform(X_one_col_res) -1 only\n",
        "y_train.tp.values = y_one_col_res.reshape(y_train.tp.values.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transform_val_test(val_test, scaler_train, is_prec=True):\n",
        "    '''\n",
        "    Input (example): ds_val_apcp.tp, scaler_train_apcp, True/False\n",
        "    Output: Transformed validation/test XR data\n",
        "    If is_prec set to True, variable is precipitation\n",
        "    '''\n",
        "    if is_prec == True:\n",
        "        X_one_col = val_test.values.reshape([val_test.values.shape[0]*val_test.values.shape[1]*val_test.values.shape[2], 1])\n",
        "        X_one_col = np.log10(X_one_col+1) \n",
        "        X_one_col_res = scaler_train.transform(X_one_col) \n",
        "        val_test.values = X_one_col_res.reshape(val_test.values.shape)\n",
        "        return val_test.values\n",
        "        \n",
        "    else:\n",
        "        X_one_col = val_test.values.reshape([val_test.values.shape[0]*val_test.values.shape[1]*val_test.values.shape[2], 1])\n",
        "        # X_one_col = np.log10(X_one_col+1) \n",
        "        X_one_col_res = scaler_train.transform(X_one_col) \n",
        "        val_test.values = X_one_col_res.reshape(val_test.values.shape)\n",
        "        return val_test.values\n",
        "\n",
        "# reforecast\n",
        "ds_val_apcp.tp.values = transform_val_test(ds_val_apcp.tp, scaler_train_apcp, True) # transforming val based on train\n",
        "ds_test_apcp.tp.values = transform_val_test(ds_test_apcp.tp, scaler_train_apcp, True) # transforming val based on train\n",
        "\n",
        "# reanalysis\n",
        "y_val.tp.values = transform_val_test(y_val.tp, scaler_train_tp, True)\n",
        "y_test.tp.values = transform_val_test(y_test.tp, scaler_train_tp, True)\n",
        "\n",
        "def inverse_val_test(transformed_vt, scaler_train, is_prec=True):\n",
        "    '''\n",
        "    Input (example): ds_val_apcp.tp, scaler_train_apcp, True/False\n",
        "    Output: Inversed of transformed validation/test XR data\n",
        "    If is_prec set to True, variable is precipitation\n",
        "    '''\n",
        "    if is_prec == True:\n",
        "        X_one_col = transformed_vt.values.reshape([transformed_vt.values.shape[0]*transformed_vt.values.shape[1]*transformed_vt.values.shape[2], 1])\n",
        "        X_one_col_res = 10**scaler_train.inverse_transform(X_one_col) -1\n",
        "        transformed_vt.values = X_one_col_res.reshape(transformed_vt.values.shape)\n",
        "        return transformed_vt.values\n",
        "    \n",
        "    else:\n",
        "        X_one_col = transformed_vt.values.reshape([transformed_vt.values.shape[0]*transformed_vt.values.shape[1]*transformed_vt.values.shape[2], 1])\n",
        "        X_one_col_res = scaler_train.inverse_transform(X_one_col)\n",
        "        transformed_vt.values = X_one_col_res.reshape(transformed_vt.values.shape)\n",
        "        return transformed_vt.values\n",
        "\n",
        "# retrieving back original precipitation \n",
        "# do not use this yet\n",
        "# ds_val_apcp.tp.values = inverse_val_test(ds_val_apcp.tp, scaler_train_apcp, True) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Checking if coordinates match the values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ds_train_apcp.tp[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ds_train_apcp.tp[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ds_train_apcp.sel(latitude=2.25, longitude=103).isel(time=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Converting to npy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(20457, 8, 11, 1)\n",
            "(4381, 8, 11, 1)\n",
            "(4381, 8, 11, 1)\n"
          ]
        }
      ],
      "source": [
        "# training\n",
        "ds_train_apcp = ds_train_apcp.tp.values\n",
        "ds_train_apcp = ds_train_apcp[..., np.newaxis]\n",
        "print(ds_train_apcp.shape)\n",
        "np.save('C:\\\\Users\\\\bobby\\\\Desktop\\\\.vscode\\\\1 UROP Research\\\\UROP v2\\\\data\\\\X_train_apcp.npy', ds_train_apcp)\n",
        "\n",
        "# val\n",
        "ds_val_apcp = ds_val_apcp.tp.values\n",
        "ds_val_apcp = ds_val_apcp[..., np.newaxis]\n",
        "print(ds_val_apcp.shape)\n",
        "np.save('C:\\\\Users\\\\bobby\\\\Desktop\\\\.vscode\\\\1 UROP Research\\\\UROP v2\\\\data\\\\X_val_apcp.npy', ds_val_apcp)\n",
        "\n",
        "# testing\n",
        "ds_test_apcp = ds_test_apcp.tp.values\n",
        "ds_test_apcp = ds_test_apcp[..., np.newaxis]\n",
        "print(ds_test_apcp.shape)\n",
        "np.save('C:\\\\Users\\\\bobby\\\\Desktop\\\\.vscode\\\\1 UROP Research\\\\UROP v2\\\\data\\\\X_test_apcp.npy', ds_test_apcp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(20457, 8, 11, 1)\n",
            "(4381, 8, 11, 1)\n",
            "(4381, 8, 11, 1)\n"
          ]
        }
      ],
      "source": [
        "# training FOR REANALYSIS \n",
        "y_train = y_train.tp.values\n",
        "y_train = y_train[..., np.newaxis]\n",
        "print(y_train.shape)\n",
        "np.save('C:\\\\Users\\\\bobby\\\\Desktop\\\\.vscode\\\\1 UROP Research\\\\UROP v2\\\\data\\\\y_train_tp.npy', y_train)\n",
        "\n",
        "# val\n",
        "y_val = y_val.tp.values\n",
        "y_val = y_val[..., np.newaxis]\n",
        "print(y_val.shape)\n",
        "np.save('C:\\\\Users\\\\bobby\\\\Desktop\\\\.vscode\\\\1 UROP Research\\\\UROP v2\\\\data\\\\y_val_tp.npy', y_val)\n",
        "\n",
        "# testing\n",
        "y_test = y_test.tp.values\n",
        "y_test = y_test[..., np.newaxis]\n",
        "print(y_test.shape)\n",
        "np.save('C:\\\\Users\\\\bobby\\\\Desktop\\\\.vscode\\\\1 UROP Research\\\\UROP v2\\\\data\\\\y_test_tp.npy', y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# log transform and min_max normalization for reanalysis, MIGHT BE USED FOR CLASSIFICATION\n",
        "# scaler_tp = MinMaxScaler()\n",
        "# X_one_col = ds_reanalysis_tp.tp.values.reshape([ds_reanalysis_tp.tp.values.shape[0]*ds_reanalysis_tp.tp.values.shape[1]*ds_reanalysis_tp.tp.values.shape[2], 1])\n",
        "# X_one_col = np.log10(X_one_col+1) # 10**X_one_col - 1 to scale back\n",
        "# X_one_col_res = scaler_tp.fit_transform(X_one_col) # scaler_tp.inverse_transform(X_one_col_res) to scale back, or use 10**scaler_tp.inverse_transform(X_one_col_res) -1 only\n",
        "# ds_reanalysis_tp.tp.values = X_one_col_res.reshape(ds_reanalysis_tp.tp.values.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ds_reanalysis_tp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Checks for REFORECAST\n",
        "# ds_reforecast_apcp = xr.open_dataset('GEFSv12-Reforecast_apcp_2000_2019.nc')\n",
        "# ds_reforecast_apcp = ds_reforecast_apcp.to_dataframe()['tp']\n",
        "# ds_reforecast_apcp = ds_reforecast_apcp.loc[~ds_reforecast_apcp.index.duplicated(),:].unstack(level=[0,1])\n",
        "# ds_reforecast_apcp = ds_reforecast_apcp.resample('D').mean()\n",
        "# selected_index = ds_reforecast_apcp.index[np.where(ds_reforecast_apcp.isnull())[0]]\n",
        "# missing_index = selected_index[np.where(selected_index.duplicated()==False)[0]]\n",
        "\n",
        "# Checks for REANALYSIS\n",
        "# ds_reanalysis_tp = xr.open_dataset('GEFSv12-Reanalysis_tp_2000_2019.nc')\n",
        "# ds_reanalysis_tp = ds_reanalysis_tp.to_dataframe()['tp']\n",
        "# ds_reanalysis_tp = ds_reanalysis_tp.loc[~ds_reanalysis_tp.index.duplicated(),:].unstack(level=[2,1])\n",
        "# ds_reanalysis_tp = ds_reanalysis_tp.resample('D').mean()\n",
        "# selected_index = ds_reanalysis_tp.index[np.where(ds_reanalysis_tp.isnull())[0]]\n",
        "# missing_index = selected_index[np.where(selected_index.duplicated()==False)[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plotting check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Looking at the magnitude and general pattern of both reforecast and reanalysis \n",
        "# 1 YEAR SAMPLE \n",
        "# lat = 2\n",
        "# lon = 101.5\n",
        "# ds_reanalysis_tp.isel(time=slice(0,1468)).groupby(ds_reanalysis_tp.time[0:1468].dt.month).mean().tp.sel(lat=lat, lon=lon).plot()\n",
        "# ds_reforecast_apcp.groupby(ds_reforecast_apcp.time.dt.month).mean().tp.sel(latitude=lat, longitude=lon).plot();\n",
        "\n",
        "# FULL 20 YEARS\n",
        "# from turtle import color\n",
        "\n",
        "# lat = 3\n",
        "# lon = 104\n",
        "# ds_reanalysis_tp.groupby(ds_reanalysis_tp.time.dt.month).mean().tp.sel(lat=lat, lon=lon).plot(color='red', alpha=0.5)\n",
        "# ds_reforecast_apcp.groupby(ds_reforecast_apcp.time.dt.month).mean().tp.sel(latitude=lat, longitude=lon).plot(color='blue', alpha=0.5);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# y = ds_reanalysis_tp.tp.values.astype(float) # 29220, 16, 19\n",
        "# y = y[~np.isnan(y)]\n",
        "# np.quantile(y, 0.95)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(not ds_reforecast_apcp.tp.isnull().any()) # True if contains all numeric values\n",
        "# print(not ds_reanalysis_tp.tp.isnull().any()) # True if contains all numeric values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reanalysis 2 x 3 grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ds_target_6 = ds_target.sel(lat= [1.25, 1.5], lon= [103.5, 103.75, 104])\n",
        "# # ds_target_6 = ds_target.sel(lat= [1.25, 1.5], lon= [103.75, 104])\n",
        "# # ds_prec_6\n",
        "\n",
        "# ds_target_6.isel(time=3).tp.plot() # 2 x 3\n",
        "\n",
        "# # ds_target_6.tp[1].sel(lat=1.25, lon=103.75) # bottom left\n",
        "# ds_target_6.tp[1].sel(lat=1.5, lon=103.75) # top left\n",
        "\n",
        "# ds_target_6.tp[1] # as one can observe, 9.1 doesnt correspond to the array properly, where it is shown at the bottom row instead of the 1st row"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reversing the lat coords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ds_target_6 = ds_target_6.isel(lat=slice(None,None,-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ds_target_6.tp[1].sel(lat=1.25, lon=103.75) # top left"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ds_target_6.tp[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Old reanalysis nc file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ds_sample = xr.open_dataset('combined-reanalysis.nc')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ds_sample.tp.values[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ds_sample.isel(time=3).tp.plot() # 2 x 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ds_sample.tp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exporting into .npy files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4R3883UdzLSN",
        "outputId": "be3295bd-0bb2-4ebc-9573-782ecbbe12ca"
      },
      "outputs": [],
      "source": [
        "# data = ds_target.tp.values\n",
        "# data = data[..., np.newaxis]\n",
        "# # data.reshape(7305,2,2,1)\n",
        "# # np.save('data.npy', data)\n",
        "# data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ5t1CinzNGG"
      },
      "source": [
        "# Combining 3D arrays into 1 single 4D array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "9wDhoQuMzMVt"
      },
      "outputs": [],
      "source": [
        "# x = np.zeros((1000, 90, 135)) # 1000 samples, 90 lat, 135 lon\n",
        "# x1 = x[..., np.newaxis]\n",
        "# print(x1.shape) # for 1 variable\n",
        "\n",
        "# y = np.stack((x, x, x), axis = -1) # stacking 3 variables into 1 single 4D array\n",
        "# print(y.shape)\n",
        "# # y2 = np.stack((x, x, x))\n",
        "# # y2.shape\n",
        "# # y.shape\n",
        "# # np.concatenate(x, axis=0)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.1 ('research')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "287ad1a523a1a0c79ba273026018d677ef26436c7e0e825f5e9b183804324b70"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
